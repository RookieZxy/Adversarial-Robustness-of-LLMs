# Safety of LLMs<span id="head"/>
A collection of papers and resources about Safety of Large Language Models (LLMs)

## Table of Contents<span id="contents"/>
* [Safety of LLMs](#head)
  * [Table of Contents](#contents)
  * [Benchmark](#benchmark)
  * [Prompting Engineering](#prompting)
    * [ICL-enhanced LLMs](#ICL-enhanced-LLMs)
    * [Zero shot](#zero)
      * [Attack](#z-attack) 
    * [Few shots](#few)
      * [Instability of ICL](#Instability-of-ICL)
    * [Defence](#defence)
  * [Blog](#blog)

## Benchmark<span id="benchmark"/>
* PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts (Arxiv, 2023) (arxiv)[pdf](https://arxiv.org/pdf/2306.04528.pdf)

## Prompting Engineering<span id="prompting"/>
### ICL-enhanced LLMs<span id="ICL-enhanced-LLMs"/>
* Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? (EMNLP)[pdf](https://arxiv.org/pdf/2202.12837.pdf)
* Emergent Abilities of Large Language Models (TMLR) [pdf](https://arxiv.org/pdf/2206.07682.pdf)
* What Makes Good In-Context Examples for GPT-3? (arxiv) [pdf](https://arxiv.org/pdf/2101.06804.pdf)
* Language Models are Few-Shot Learners (arxiv) [pdf](https://arxiv.org/pdf/2005.14165.pdf)
* Impact of Pretraining Term Frequencies on Few-Shot Reasoning (arxiv) [pdf](https://arxiv.org/pdf/2202.07206.pdf)
### Zero shot<span id="zero"/>
#### &emsp;Attack<span id="z-attack"/>
* Universal and Transferable Adversarial Attacks on Aligned Language Models (arxiv) [pdf](https://arxiv.org/pdf/2307.15043.pdf)
* Universal and Transferable Adversarial Attacks (arxiv)[pdf](https://arxiv.org/pdf/2307.15043.pdf)
* Adversarial Prompting for Black Box Foundation Models (arxiv) [pdf](https://arxiv.org/abs/2302.04237)
### Few shots <span id="few"/>

####  &emsp;Instability of ICL<span id="Instability-of-ICL"/>
* Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning (arxiv) [pdf](https://arxiv.org/pdf/2305.17256.pdf)
* Adversarial Demonstration Attacks on Large Language Models (arxiv) [pdf](https://arxiv.org/pdf/2305.14950.pdf)
  
### Defence <span id="defence"/>
* What is Perplexity? [link](https://lukesalamone.github.io/posts/perplexity/)
* Baseline Defenses for Adversarial Attacks Against Aligned Language Models (arxiv)[paper](https://arxiv.org/pdf/2309.00614.pdf)
## Blog<span id="blog"/>
* Prompt Engineering [blog](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)
