# Safty-of-LLMs

A collection of papers and resources about adversarial robustness of In context learning(ICL)

With emergence of Large Language models(LLMs), the parameter of the LLMs explosive growth. For instance the Llama2, which is published by Meta, is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Therefore fine tuning LLMs become time and money-consuming. In-context learning(ICL), which don't touch the weight of model, becomes a powerfult paradigm leveraging LLMs for specific task by utilizing data-label paired examples as demonstrations in the precondition prompts. Despite it's promising performance, the stability of ICL is affected by selection and organization of demonstrations.

# Overview
In this repository, we will introduce the ICL and collect recent advances in safety of LLMs especially for In-Context Learning. 
We hope this repository can help researchers to get better understanding of this promising field.

# Table of Contents

# What is ICL

# Attack
## Benchmark 
  PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts (Arxiv, 2023) [paper](https://arxiv.org/pdf/2306.04528.pdf).

    

