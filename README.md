# Safety-of-LLMs<span id="head"/>

A collection of papers and resources about adversarial robustness of In context learning(ICL)

With emergence of Large Language models(LLMs), the parameter of the LLMs explosive growth. For instance the Llama2, which is published by Meta, is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Therefore fine tuning LLMs become time and money-consuming. In-context learning(ICL), which don't touch the weight of model, becomes a powerfult paradigm leveraging LLMs for specific task by utilizing data-label paired examples as demonstrations in the precondition prompts. Despite it's promising performance, the stability of ICL is affected by selection and organization of demonstrations.

## Overview
In this repository, we will introduce the ICL and collect recent advances in safety of LLMs especially for In-Context Learning. 
We hope this repository can help researchers to get better understanding of this promising field.

## Table of Contents
* [Safety of LLMs](#head)
    * [Overview](##Overview)
    * [Table of Contents](##table-of-contents)
    * [Related Studys](##Related Studys)
    * [What is ICL)](##What is ICL])
    * [ICL-enhanced LLMs](##ICL-enhanced LLMs)
    * [Unrobustness of ICL](##Unrobustness of ICL)
        * [Benchmark](###Benchmark)
        * [Attacks based on searching method](###Attacks based on searching method)
        * [Attacks based on learning method](###Attacks based on learning method)

## Related Studys
* Adversarial Demonstration Attacks on Large Language Models [paper](https://arxiv.org/pdf/2305.14950.pdf)

## What is ICL

## ICL-enhanced LLMs

## Unrobustness of ICL

### Benchmark 
* PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts (Arxiv, 2023) [paper](https://arxiv.org/pdf/2306.04528.pdf)
### Attacks based on searching method
* Universal and Transferable Adversarial Attacks on Aligned Language Models [paper](https://arxiv.org/pdf/2307.15043.pdf)
* Large Language Models Can be Lazy Learners Analyze Shortcuts in [paper](https://arxiv.org/pdf/2305.17256.pdf)
### Attacks based on learning method
* Universal and Transferable Adversarial Attacks [paper](https://arxiv.org/pdf/2307.15043.pdf)


    

